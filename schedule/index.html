<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link href="/tpm2023/css/franklin.css" rel=stylesheet > <link href="/tpm2023/css/vela.css" rel=stylesheet > <link href="/tpm2023/css/style.css" rel=stylesheet > <script src="/tpm2023/libs/vela/jquery.min.js"></script> <meta name=format-detection  content="telephone=no" /> <link rel=stylesheet  href="/tpm2023/libs/bootstrap/bootstrap.min.css"> <link href="/tpm2023/libs/fontawesome/css/fontawesome.css" rel=stylesheet > <link href="/tpm2023/libs/fontawesome/css/brands.css" rel=stylesheet > <link href="/tpm2023/libs/fontawesome/css/solid.css" rel=stylesheet > <title>TPM 2023</title> <div class="main-nav slideout-menu slideout-menu-left" id=menu > <div class=flex-container > <span class=sidebar-brand > <h3 style='font-size: 25px'>TPM 2023</h3> </span> </div> <nav class=sidebar-nav > <ul class=metismenu  id=metismenu  > <li><a href="/tpm2023/index.html">Home</a> <li><a href="/tpm2023/cfp/">Call for papers</a> <li><a href="/tpm2023/papers/">Accepted papers</a> <li><a href="/tpm2023/schedule/">Schedule</a> </ul> </nav> </div> <main id=panel  class="slidout-panel slideout-panel-left"> <div class="toggle-button hamburger hamburger--spin"> <div class=hamburger-box > <div class=hamburger-inner ></div> </div> </div> <h1 class="page title">Schedule</h1> <h2 class="page title"><small> </small></h2> <h2><center></center></h2> <hr> <div class=franklin-content ><p>The workshop will be held <strong>in person</strong> on August 4th, 2023 in Pittsburgh &#40;PA, USA&#41; at the Carnegie Mellon University.<br />Room: McKenna, Peter, and Wright Room. <strong>In-person poster session</strong> will take place in the same room.<br/></p> <p>The workshop will be <strong>streamed</strong> via Zoom at <a href="https://tinyurl.com/UAI23MPW">this link</a>. The <strong>online poster session</strong> will take place via Zoom at <a href="https://tinyurl.com/UAI23-TPM">this link</a>.<br />Please, refer to the UAI conference instructions and contact the UAI online chairs for any issues.</p> <h1 id=workshop_schedule ><a href="#workshop_schedule" class=header-anchor >Workshop Schedule</a></h1> <table><tr><th align=right >Time<th align=right >Description<tr><td align=right >9:00 – 9:10 am<td align=right >Welcome &amp; Best Paper Awards<tr><td align=right >9:10 – 9:35 am<td align=right >Spotlight Presentations &#40;part 1&#41;<tr><td align=right >9:35 – 10:00 am<td align=right >Causal normalizing flows: from theory to practice <br/><em>Adrián Javaloy, Pablo Sanchez Martin, Isabel Valera</em><tr><td align=right >10:00 – 10:30 am<td align=right >Coffee Break &amp; Virtual Poster Session<tr><td align=right >10:30 – 11:30 am<td align=right >Bayesian Learning Rule <br/> Speaker: <em>Emtiyaz Khan</em><tr><td align=right >11:30 – 12:30 pm<td align=right >Tractable Molecule Generation and Beyond with Transformers <br/> Speaker: <em>Jakub M. Tomczak</em><tr><td align=right >12:30 – 2:00 pm<td align=right >Lunch Break &amp; Virtual Poster Session<tr><td align=right >2:00 – 3:00 pm<td align=right >Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box <br/> Speaker: <em>Tamara Broderick</em><tr><td align=right >3:00 – 3:25 am<td align=right >Spotlight Presentations &#40;part 2&#41;<tr><td align=right >3:25 – 3:50 pm<td align=right >Training and Inference on Any-Order Autoregressive Models the Right Way <br/><em>Andy Shih, Dorsa Sadigh, Stefano Ermon</em><tr><td align=right >3:50 – 4:30 pm<td align=right >Coffee Break &amp; Virtual Poster Session<tr><td align=right >4:30 – 6:30 pm<td align=right >In-person Poster Session &#40;McKenna, Peter, and Wright Room&#41; <br/> Virtual Poster Session &#40;<a href="https://tinyurl.com/UAI23-TPM">Zoom link</a>&#41;</table> <h1 id=invited_talks ><a href="#invited_talks" class=header-anchor >Invited Talks</a></h1> <ul> <li> <h2> Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box </h2> <p><strong>Tamara Broderick</strong> (MIT, USA) <p/> <p><strong>Abstract: </strong>Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor uncertainty estimates of mean-field variational Bayes (MFVB). We introduce 'deterministic ADVI' (D-ADVI) to solve these issues. In particular, we replace the intractable MFVB objective with a Monte Carlo approximation; subsequently fixing the Monte Carlo draws allows the use of off-the-shelf deterministic optimization tools. We show that D-ADVI reliably finds good solutions with default settings (unlike ADVI) and is faster and more accurate than ADVI. Moreover, unlike ADVI, D-ADVI is amenable to linear response corrections, yielding more accurate posterior covariance estimates. We demonstrate the benefits of D-ADVI on a variety of real-world problems. <p/> <div class=text-muted ><p><strong>Bio: </strong>Tamara Broderick is an Associate Professor in the Department of Electrical Engineering and Computer Science at MIT. She is a member of the MIT Laboratory for Information and Decision Systems (LIDS), the MIT Statistics and Data Science Center, and the Institute for Data, Systems, and Society (IDSS). She completed her Ph.D. in Statistics at the University of California, Berkeley in 2014. Previously, she received an AB in Mathematics from Princeton University (2007), a Master of Advanced Study for completion of Part III of the Mathematical Tripos from the University of Cambridge (2008), an MPhil by research in Physics from the University of Cambridge (2009), and an MS in Computer Science from the University of California, Berkeley (2013). Her recent research has focused on developing and analyzing models for scalable Bayesian machine learning. She has been awarded selection to the COPSS Leadership Academy (2021), an Early Career Grant (ECG) from the Office of Naval Research (2020), an AISTATS Notable Paper Award (2019), an NSF CAREER Award (2018), a Sloan Research Fellowship (2018), an Army Research Office Young Investigator Program (YIP) award (2017), Google Faculty Research Awards, an Amazon Research Award, the ISBA Lifetime Members Junior Researcher Award, the Savage Award (for an outstanding doctoral dissertation in Bayesian theory and methods), the Evelyn Fix Memorial Medal and Citation (for the Ph.D. student on the Berkeley campus showing the greatest promise in statistical research), the Berkeley Fellowship, an NSF Graduate Research Fellowship, a Marshall Scholarship, and the Phi Beta Kappa Prize (for the graduating Princeton senior with the highest academic average). <p/></div> <li> <h2> Bayesian Learning Rule </h2> <p><strong>Emtiyaz Khan</strong> (RIKEN-AIP, Japan) <p/> <p><strong>Abstract: </strong>Humans and animals have a natural ability to autonomously learn and quickly adapt to their surroundings. How can we design machines that do the same? In this talk, I will present Bayesian principles to bridge such gaps between humans and machines. I will show that a wide-variety of machine-learning algorithms are instances of a single learning-rule derived from Bayesian principles. I will also briefly discuss the dual perspective yielding new mechanisms for knowledge transfer in learning machines. My hope is to convince the audience that Bayesian principles are indispensable for an AI that learns as efficiently as we do. <p/> <div class=text-muted ><p><strong>Bio: </strong>Emtiyaz Khan (also known as Emti) is a (tenured) team leader at the RIKEN center for Advanced Intelligence Project (AIP) in Tokyo where he leads the Approximate Bayesian Inference Team. Previously, he was a postdoc and then a scientist at Ecole Polytechnique Fédérale de Lausanne (EPFL), where he also taught two large machine learning courses and received a teaching award. He finished his PhD in machine learning from University of British Columbia in 2012. The main goal of Emti’s research is to understand the principles of learning from data and use them to develop algorithms that can learn like living beings. For more than 10 years, his work has focused on developing Bayesian methods that could lead to such fundamental principles. The approximate Bayesian inference team now continues to use these principles, as well as derive new ones, to solve real-world problems. <p/></div> <li> <h2> Tractable Molecule Generation and Beyond with Transformers </h2> <p><strong>Jakub M. Tomczak</strong> (TU/e, NL) <p/> <p><strong>Abstract: </strong>ChatGPT and Stable Diffusion showed that Generative AI is the way to go in building a new generation of AI systems. But what about other applications, beyond images and text? What about the tractability of these models? In this talk, we will focus on molecular modeling and discuss how to formulate tractable deep generative models for molecule generation and de novo drug design. <p/> <div class=text-muted ><p><strong>Bio: </strong>Jakub M. Tomczak is an associate professor (a PI of the Generative AI group) at the Eindhoven University of Technology. Before, he was an assistant professor of Artificial Intelligence at the Vrije Universiteit Amsterdam, a deep learning researcher (Engineer, Staff) in Qualcomm AI Research in Amsterdam, a Marie Sklodowska-Curie individual fellow in Prof. Max Welling's group at the University of Amsterdam, and an assistant professor and postdoc at the Wroclaw University of Technology. His main research interests include deep generative modeling, deep learning, and Bayesian inference, with applications to image processing, robotics, and life sciences. He is the author of the book entitled “Deep Generative Modeling”. He is also the founder of Amsterdam AI Solutions. <p/></div> </ul> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Last modified: August 04, 2023. </div> </div> </main> <script src="/tpm2023/libs/vela/metisMenu.min.js"></script> <script src="/tpm2023/libs/vela/slideout.min.js"></script>